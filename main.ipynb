{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP + SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from segment_anything import build_sam, build_sam_vit_b, SamAutomaticMaskGenerator\n",
    "from PIL import Image, ImageDraw\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model weights to load them here\n",
    "mask_generator = SamAutomaticMaskGenerator(build_sam_vit_b(checkpoint=\"./model_checkpoints/sam_vit_b_01ec64.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"assets/example-image.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "masks = mask_generator.generate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_box_xywh_to_xyxy(box):\n",
    "    x1 = box[0]\n",
    "    y1 = box[1]\n",
    "    x2 = box[0] + box[2]\n",
    "    y2 = box[1] + box[3]\n",
    "    return [x1, y1, x2, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(image, segmentation_mask):\n",
    "    image_array = np.array(image)\n",
    "    segmented_image_array = np.zeros_like(image_array)\n",
    "    segmented_image_array[segmentation_mask] = image_array[segmentation_mask]\n",
    "    segmented_image = Image.fromarray(segmented_image_array)\n",
    "    black_image = Image.new(\"RGB\", image.size, (0, 0, 0))\n",
    "    transparency_mask = np.zeros_like(segmentation_mask, dtype=np.uint8)\n",
    "    transparency_mask[segmentation_mask] = 255\n",
    "    transparency_mask_image = Image.fromarray(transparency_mask, mode='L')\n",
    "    black_image.paste(segmented_image, mask=transparency_mask_image)\n",
    "    return black_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut out all masks\n",
    "image = Image.open(image_path)\n",
    "cropped_boxes = []\n",
    "\n",
    "for mask in masks:\n",
    "    cropped_boxes.append(segment_image(image, mask[\"segmentation\"]).crop(convert_box_xywh_to_xyxy(mask[\"bbox\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_image(image, masks[4]['segmentation']).crop(convert_box_xywh_to_xyxy(masks[4]['bbox']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cropped_images\n",
    "base_path_for_saving = './output/cropped_images'\n",
    "os.makedirs(base_path_for_saving, exist_ok=True)\n",
    "for i, cropped_img in enumerate(cropped_boxes):\n",
    "    filename = f'cropped_{i}.jpg' \n",
    "    cropped_img.save(os.path.join(base_path_for_saving, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def retriev(elements: list[Image.Image], search_text: str) -> int:\n",
    "    preprocessed_images = [preprocess(image).to(device) for image in elements]\n",
    "    tokenized_text = clip.tokenize([search_text]).to(device)\n",
    "    stacked_images = torch.stack(preprocessed_images)\n",
    "    image_features = model.encode_image(stacked_images)\n",
    "    text_features = model.encode_text(tokenized_text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    probs = 100. * image_features @ text_features.T\n",
    "    return probs[:, 0].softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices_of_values_above_threshold(values, threshold):\n",
    "    return [i for i, v in enumerate(values) if v > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = retriev(cropped_boxes, \"cherry\")\n",
    "indices = get_indices_of_values_above_threshold(scores, 0.05)\n",
    "\n",
    "segmentation_masks = []\n",
    "\n",
    "for seg_idx in indices:\n",
    "    segmentation_mask_image = Image.fromarray(masks[seg_idx][\"segmentation\"].astype('uint8') * 255)\n",
    "    segmentation_masks.append(segmentation_mask_image)\n",
    "\n",
    "original_image = Image.open(image_path)\n",
    "overlay_image = Image.new('RGBA', image.size, (0, 0, 0, 0))\n",
    "overlay_color = (255, 0, 0, 200)\n",
    "\n",
    "draw = ImageDraw.Draw(overlay_image)\n",
    "for segmentation_mask_image in segmentation_masks:\n",
    "    draw.bitmap((0, 0), segmentation_mask_image, fill=overlay_color)\n",
    "\n",
    "result_image = Image.alpha_composite(original_image.convert('RGBA'), overlay_image)\n",
    "# result_image\n",
    "result_image.convert('RGB').save('./output/final_result.jpg')\n",
    "result_image.convert('RGB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPRNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
